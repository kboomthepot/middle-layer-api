// worker.js
const express = require('express');
const bodyParser = require('body-parser');
const { BigQuery } = require('@google-cloud/bigquery');

// ---------- CONFIG ----------
const PROJECT_ID = 'ghs-construction-1734441714520';

// Main jobs table (source)
const DATASET_ID = 'Client_audits';
const JOBS_TABLE_ID = 'client_audits_jobs';

// Demographics data source
const DEMOS_DATASET_ID = 'Client_audits_data';
const DEMOS_SOURCE_TABLE_ID = '1_demographics';

// Demographics jobs table (target)
const JOBS_DEMOS_TABLE_ID = 'jobs_demographics';

const bigquery = new BigQuery({ projectId: PROJECT_ID });

const app = express();
app.use(bodyParser.json());

// ---------- HEALTH CHECK ----------
app.get('/', (req, res) => {
  res.send('Worker service listening on port 8080');
});

// ---------- PUB/SUB ENDPOINT ----------
app.post('/', async (req, res) => {
  try {
    const envelope = req.body;
    if (!envelope || !envelope.message || !envelope.message.data) {
      console.error('âŒ Invalid Pub/Sub message format:', JSON.stringify(envelope));
      // ACK anyway so Pub/Sub doesnâ€™t retry forever
      return res.status(204).send();
    }

    const payload = JSON.parse(
      Buffer.from(envelope.message.data, 'base64').toString()
    );

    console.log('ðŸ“© Received job message:', payload);

    const { jobId, location: locationFromMessage } = payload;

    console.log(
      `âœ… Worker received job ${jobId} (location=${locationFromMessage || 'N/A'})`
    );

    if (!jobId) {
      console.error('âŒ [DEMOS] Missing jobId in message payload. Skipping.');
      return res.status(204).send();
    }

    await processJobDemographics(jobId, locationFromMessage || null);

    // Always ACK so Pub/Sub does not retry this message
    res.status(204).send();
  } catch (err) {
    console.error('âŒ Error handling Pub/Sub message:', err);
    // ACK even on error to avoid infinite retry loop
    res.status(204).send();
  }
});

// ---------- DEMOGRAPHICS PROCESSOR ----------

async function processJobDemographics(jobId, locationFromMessage = null) {
  console.log(`â–¶ï¸ [DEMOS] Starting demographics processing for job ${jobId}`);

  // ---- Step 1: Load job row from client_audits_jobs ----
  console.log('âž¡ï¸ [DEMOS] Step 1: Load job row from client_audits_jobs');

  const [jobRows] = await bigquery.query({
    query: `
      SELECT
        jobId,
        location,
        demographicsStatus,
        paidAdsStatus,
        status,
        createdAt
      FROM \`${PROJECT_ID}.${DATASET_ID}.${JOBS_TABLE_ID}\`
      WHERE jobId = @jobId
      LIMIT 1
    `,
    params: { jobId },
  });

  console.log(`â„¹ï¸ [DEMOS] Step 1 result rows: ${jobRows.length}`);

  if (!jobRows.length) {
    console.warn(
      `âš ï¸ [DEMOS] Job ${jobId} not found in ${DATASET_ID}.${JOBS_TABLE_ID}.`
    );
    return;
  }

  const job = jobRows[0];
  const paidAdsStatus = job.paidAdsStatus || null;
  const location = job.location || locationFromMessage || null;

  // Normalize createdAt from jobs table (will be stored as "date" in jobs_demographics)
  const jobCreatedAtIso = job.createdAt
    ? new Date(job.createdAt).toISOString()
    : null;

  console.log(
    `â„¹ï¸ [DEMOS] Job ${jobId} location = "${location}", demographicsStatus = ${job.demographicsStatus}, paidAdsStatus = ${paidAdsStatus}, status = ${job.status}, createdAt=${jobCreatedAtIso}`
  );

  if (!location) {
    console.warn(
      `âš ï¸ [DEMOS] Job ${jobId} has no location; cannot process demographics.`
    );

    // Mark main job as failed for demographics
    await markJobDemographicsStatus(jobId, 'failed');
    return;
  }

  // ---- Step 2: Mark main job demographicsStatus = pending ----
  console.log('âž¡ï¸ [DEMOS] Step 2: Mark main job status = pending');

  try {
    const [updateJob] = await bigquery.createQueryJob({
      query: `
        UPDATE \`${PROJECT_ID}.${DATASET_ID}.${JOBS_TABLE_ID}\`
        SET demographicsStatus = 'pending'
        WHERE jobId = @jobId
      `,
      params: { jobId },
    });
    const [updateResult] = await updateJob.getQueryResults();
    console.log(
      `â„¹ï¸ [DEMOS] Step 2: Updated main job demographicsStatus to 'pending' for job ${jobId} (dmlStats= ${
        updateResult && updateResult[0] ? JSON.stringify(updateResult[0]) : 'null'
      }).`
    );
  } catch (err) {
    console.error(
      `âŒ [DEMOS] Error updating main job to pending for job ${jobId}:`,
      err
    );
    // don't hard-fail, continue to try demographics
  }

  // ---- Step 3: Load demographics source row ----
  console.log(
    'âž¡ï¸ [DEMOS] Step 3: Load demographics from Client_audits_data.1_demographics'
  );

  const [demoRows] = await bigquery.query({
    query: `
      SELECT
        population_no,
        median_age,
        households_no,
        median_income_households,
        median_income_families,
        male_percentage,
        female_percentage
      FROM \`${PROJECT_ID}.${DEMOS_DATASET_ID}.${DEMOS_SOURCE_TABLE_ID}\`
      WHERE location = @location
      LIMIT 1
    `,
    params: { location },
  });

  console.log(`â„¹ï¸ [DEMOS] Step 3 result rows (demographics): ${demoRows.length}`);

  if (!demoRows.length) {
    console.warn(
      `âš ï¸ [DEMOS] No demographics found in ${DEMOS_DATASET_ID}.${DEMOS_SOURCE_TABLE_ID} for location "${location}". Marking as failed.`
    );

    // Delete any existing row, then insert a "failed" row with nulls
    await overwriteJobsDemographicsRow(jobId, {
      jobId,
      location,
      population_no: null,
      median_age: null,
      median_income_households: null,
      median_income_families: null,
      male_percentage: null,
      female_percentage: null,
      status: 'failed',
      // date will be filled from jobCreatedAtIso inside helper
    }, jobCreatedAtIso);

    await markJobDemographicsStatus(jobId, 'failed');
    return;
  }

  const demo = demoRows[0];

  console.log(
    `â„¹ï¸ [DEMOS] Found demographics for "${location}": ` +
      JSON.stringify(demo)
  );

  // ---- Step 4: Compute metrics + status; overwrite jobs_demographics row via streaming insert ----
  console.log(
    'âž¡ï¸ [DEMOS] Step 4: Overwrite jobs_demographics with demographics values via streaming insert'
  );

  const parsed = {
    population_no: toNumberOrNull(demo.population_no),
    median_age: toNumberOrNull(demo.median_age),
    median_income_households: toNumberOrNull(demo.median_income_households),
    median_income_families: toNumberOrNull(demo.median_income_families),
    male_percentage: toNumberOrNull(demo.male_percentage),
    female_percentage: toNumberOrNull(demo.female_percentage),
  };

  const metricsArray = [
    parsed.population_no,
    parsed.median_age,
    parsed.median_income_households,
    parsed.median_income_families,
    parsed.male_percentage,
    parsed.female_percentage,
  ];

  const allNull = metricsArray.every((v) => v === null);
  const allNonNull = metricsArray.every((v) => v !== null);

  let newDemoStatus;
  if (allNull) newDemoStatus = 'failed';
  else if (allNonNull) newDemoStatus = 'completed';
  else newDemoStatus = 'partial';

  // Cleaner Step 4 log â€“ just the parsed values
  console.log(
    `â„¹ï¸ [DEMOS] Step 4 metrics for job ${jobId}:`,
    parsed,
    `=> newDemoStatus="${newDemoStatus}"`
  );

  // Delete any existing row for this jobId (safe DML â€“ doesn't touch streaming rows)
  try {
    const [deleteJob] = await bigquery.createQueryJob({
      query: `
        DELETE FROM \`${PROJECT_ID}.${DATASET_ID}.${JOBS_DEMOS_TABLE_ID}\`
        WHERE jobId = @jobId
      `,
      params: { jobId },
    });
    await deleteJob.getQueryResults();
    console.log(
      `â„¹ï¸ [DEMOS] Step 4: Deleted any existing jobs_demographics row for job ${jobId} before streaming insert.`
    );
  } catch (err) {
    console.error(
      `âŒ [DEMOS] Error deleting existing jobs_demographics row for job ${jobId}:`,
      err
    );
    // continue anyway; insert will just create a fresh row
  }

  // Streaming insert full row with metrics + status
  const nowIso = new Date().toISOString();
  const rowToInsert = {
    jobId,
    location,
    population_no: parsed.population_no,
    median_age: parsed.median_age,
    median_income_households: parsed.median_income_households,
    median_income_families: parsed.median_income_families,
    male_percentage: parsed.male_percentage,
    female_percentage: parsed.female_percentage,
    status: newDemoStatus,
    // ðŸ‘‡ store original job createdAt from client_audits_jobs into "date"
    date: jobCreatedAtIso,
    createdAt: nowIso,
    updatedAt: nowIso,
  };

  console.log(
    `â„¹ï¸ [DEMOS] Step 4 rowToInsert for job ${jobId}:`,
    rowToInsert
  );

  try {
    await bigquery
      .dataset(DATASET_ID)
      .table(JOBS_DEMOS_TABLE_ID)
      .insert([rowToInsert], { ignoreUnknownValues: true });

    console.log(
      `âœ… [DEMOS] Streaming insert completed for job ${jobId} into ${DATASET_ID}.${JOBS_DEMOS_TABLE_ID}`
    );
  } catch (err) {
    console.error(
      `âŒ [DEMOS] Streaming insert FAILED for job ${jobId} into jobs_demographics:`,
      JSON.stringify(err.errors || err, null, 2)
    );

    // If insert fails, mark main job as failed
    await markJobDemographicsStatus(jobId, 'failed');
    return;
  }

  // Optional: one quick verification attempt (SELECT reads streaming buffer)
  try {
    const [checkRows] = await bigquery.query({
      query: `
        SELECT
          jobId,
          location,
          population_no,
          median_age,
          median_income_households,
          median_income_families,
          male_percentage,
          female_percentage,
          status,
          date
        FROM \`${PROJECT_ID}.${DATASET_ID}.${JOBS_DEMOS_TABLE_ID}\`
        WHERE jobId = @jobId
        LIMIT 1
      `,
      params: { jobId },
    });

    const r = checkRows[0] || null;

    // ðŸ‘‡ Cleaned-up log: convert Big values to string/number and show only useful fields
    if (r) {
      console.log(
        `â„¹ï¸ [DEMOS] Step 4 check row for job ${jobId}:`,
        {
          jobId: r.jobId,
          location: r.location,
          population_no: r.population_no?.toString?.() ?? r.population_no,
          median_age: r.median_age?.toString?.() ?? r.median_age,
          median_income_households:
            r.median_income_households?.toString?.() ?? r.median_income_households,
          median_income_families:
            r.median_income_families?.toString?.() ?? r.median_income_families,
          male_percentage: r.male_percentage?.toString?.() ?? r.male_percentage,
          female_percentage:
            r.female_percentage?.toString?.() ?? r.female_percentage,
          status: r.status,
          date: r.date,
        }
      );
    } else {
      console.log(`â„¹ï¸ [DEMOS] Step 4 check row for job ${jobId}: null`);
    }
  } catch (err) {
    console.error(
      `âŒ [DEMOS] Error verifying jobs_demographics row for job ${jobId}:`,
      err
    );
  }

  // ---- Step 5: Update main job's demographicsStatus based on newDemoStatus ----
  console.log('âž¡ï¸ [DEMOS] Step 5: Update client_audits_jobs.demographicsStatus');

  await markJobDemographicsStatus(jobId, newDemoStatus);

  // ---- Step 6: Optionally update main job.status if all segments done ----
  console.log(
    'âž¡ï¸ [DEMOS] Step 6: Optionally mark main job status = completed if all segments done'
  );

  try {
    if (newDemoStatus === 'completed' && paidAdsStatus === 'completed') {
      const [statusJob] = await bigquery.createQueryJob({
        query: `
          UPDATE \`${PROJECT_ID}.${DATASET_ID}.${JOBS_TABLE_ID}\`
          SET status = 'completed'
          WHERE jobId = @jobId
        `,
        params: { jobId },
      });
      await statusJob.getQueryResults();
      console.log(
        `â„¹ï¸ [DEMOS] Step 6: Marked main job status='completed' for job ${jobId} (both demographics + paidAds completed).`
      );
    } else {
      console.log(
        `â„¹ï¸ [DEMOS] Step 6: Main job status NOT set to completed for job ${jobId} (demoStatus=${newDemoStatus}, paidAdsStatus=${paidAdsStatus}).`
      );
    }
  } catch (err) {
    console.error(
      `âŒ [DEMOS] Error updating main job status for job ${jobId}:`,
      err
    );
  }
}

// ---------- HELPERS ----------

function toNumberOrNull(value) {
  if (value === null || value === undefined || value === '') return null;
  const n = Number(value);
  if (Number.isNaN(n)) return null;
  return n;
}

async function markJobDemographicsStatus(jobId, demoStatus) {
  // demoStatus is expected: 'pending' | 'completed' | 'partial' | 'failed'
  try {
    const [job] = await bigquery.createQueryJob({
      query: `
        UPDATE \`${PROJECT_ID}.${DATASET_ID}.${JOBS_TABLE_ID}\`
        SET demographicsStatus = @demoStatus
        WHERE jobId = @jobId
      `,
      params: { jobId, demoStatus },
    });
    await job.getQueryResults();
    console.log(
      `âœ… [DEMOS] Marked demographicsStatus = '${demoStatus}' for job ${jobId}`
    );
  } catch (err) {
    console.error(
      `âŒ [DEMOS] Error marking demographicsStatus='${demoStatus}' for job ${jobId}:`,
      err
    );
  }
}

async function overwriteJobsDemographicsRow(jobId, data, jobCreatedAtIso = null) {
  // Convenience for "no data" case
  const nowIso = new Date().toISOString();
  const row = {
    jobId: data.jobId,
    location: data.location || null,
    population_no: data.population_no ?? null,
    median_age: data.median_age ?? null,
    median_income_households: data.median_income_households ?? null,
    median_income_families: data.median_income_families ?? null,
    male_percentage: data.male_percentage ?? null,
    female_percentage: data.female_percentage ?? null,
    status: data.status || 'failed',
    // ðŸ‘‡ keep date aligned with client_audits_jobs.createdAt
    date: jobCreatedAtIso,
    createdAt: nowIso,
    updatedAt: nowIso,
  };

  try {
    const [deleteJob] = await bigquery.createQueryJob({
      query: `
        DELETE FROM \`${PROJECT_ID}.${DATASET_ID}.${JOBS_DEMOS_TABLE_ID}\`
        WHERE jobId = @jobId
      `,
      params: { jobId },
    });
    await deleteJob.getQueryResults();
    console.log(
      `â„¹ï¸ [DEMOS] overwriteJobsDemographicsRow: deleted existing row(s) for job ${jobId}`
    );
  } catch (err) {
    console.error(
      `âŒ [DEMOS] overwriteJobsDemographicsRow: error deleting existing rows for job ${jobId}:`,
      err
    );
  }

  try {
    await bigquery
      .dataset(DATASET_ID)
      .table(JOBS_DEMOS_TABLE_ID)
      .insert([row], { ignoreUnknownValues: true });
    console.log(
      `âœ… [DEMOS] overwriteJobsDemographicsRow: inserted row for job ${jobId} with status=${row.status}`
    );
  } catch (err) {
    console.error(
      `âŒ [DEMOS] overwriteJobsDemographicsRow: streaming insert failed for job ${jobId}:`,
      JSON.stringify(err.errors || err, null, 2)
    );
  }
}

// ---------- START SERVER ----------
const port = process.env.PORT || 8080;
app.listen(port, () => {
  console.log(`Worker service listening on port ${port}`);
});
